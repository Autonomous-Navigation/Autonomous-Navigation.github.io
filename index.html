<!DOCTYPE html>
<html>
<head>
	<title>AGNI</title>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="YOUR_GTAG_JS_SOURCE"></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>
<body>
	<br>
	<center>
		<span style="font-size:36px">AGNI : Autonomous Ground Navigation Initiative</span>
		<table align="center" width="1000px">
			<tr>
				<td>
					<table align="center" width="1000px" cellspacing="5px">
						<tr>
							<td align="center" width="150px">
								<span style="font-size:20px"><a href="https://www.linkedin.com/in/arjun-sivakumar-2459701a0/" target="_blank">Arjun Sivakumar<sup>1</sup>*</a></span>
							</td>
							<td align="center" width="150px">
								<span style="font-size:20px"><a href="https://www.linkedin.com/in/bhumil-depani-779a32147/" target="_blank">Bhumil Depani<sup>1</sup>*</a></span>
							</td>
							<td align="center" width="150px">
								<span style="font-size:20px"><a href="https://www.linkedin.com/in/ricky-bevan-babu-290ba9190/" target="_blank">Ricky Bevan Babu<sup>1</sup>*</a></span>
							</td>
						</tr>
					</table>
				</td>
			</tr>
		</table>
		<table align="center" width="800px">
                <tr>
                    <td>
                        <center>
                            <img class="round" style="width:500px" src="files/css/image/Screenshot_20230524_115532_Gallery-removebg-preview.png"/>
                        </center>
			<center>
                            <img class="round" style="width:500px" src="files/css/image/team.jpg"/>
                        </center>    
                    </td>
                </tr>
            </table>

		<br>

		<table align="center" width="850px">
		<h1 style="text-align:center">Problem Statement</h1>
		<tr>
			<td>
				<ul>
					<li>Last-mile delivery challenges in urban environments include traffic congestion, delays, and environmental impacts, which hinder efficient operations.</li>
                                        <li>Existing solutions like rovers and drones face limitations in navigating rough terrain, stairs, and urban complexities, and have restricted operational ranges.</li>
                                        <li>A comprehensive solution integrating the strengths of both rovers and drones is needed to address these challenges, ensuring adaptable, safe, and efficient deliveries.</li>
					
					
				</ul>
			</td>
		</tr>
	</table>

	
	<table align="center" width="850px">
		<h1 style="text-align:center">Project Overview</h1>
		<tr>
			<td>
				<ul>
					<li>The project focuses on processing data streams from various sensors to command a rover for autonomous movement.</li>
                                        <li>Utilizes the Pixhawk Cube as a flight controller running Ardupilot software, receiving inputs from a Jetson Nano companion computer via Dronekit-Python API and MAVLink-MAVProxy protocol.</li>
                                        <li>Integrates sensors like the Intel Realsense D455 depth camera and Slamtec RPLiDAR A3 with ROS on the Jetson Nano for navigation, implementing advanced algorithms for data processing and fusion.</li>
                                        <li>Employs techniques like Semantic Segmentation, Object Detection, and Extended Kalman filters to maneuver the rover, using OpenRouteService API for waypoint navigation.</li>
                               </ul>
			</td>
		</tr>
	</table>
	<br>
	
	<center><h1>Hardware</h1></center>
<table align="center" style="width:60%;">
    <tr>
        <td>
            <ul>
                <li><b>Pixhawk Cube (Flight Controller)</b></li>
                <li>The Pixhawk Cube is a control device for unmanned vehicles, guiding them and processing sensor information.</li>
                <li>It uses built-in sensors to understand the vehicle's location and surroundings.</li>
                <li>The Cube turns navigation commands into specific motor movements for accurate and autonomous travel.</li>
                <figure align="center">
                    <img style="width:500px" src="files/css/image/Pixhawk Cube .jpg"/>
                </figure>
            </ul>
        </td>
    </tr>
    <tr>
        <td>
            <ul>
                <li><b>Here2 (GPS Receiver)</b></li>
                <li>A GPS receiver is essential for autonomous robots, providing accurate location data for precise navigation on Earth.</li>
                <li>It allows for route planning and real-time position updates, essential for efficient and reliable autonomous movement.</li>
                <li>The Here2 GPS receiver offers high precision, utilizing multiple satellite systems for centimeter-level location accuracy.</li>
                <figure align="center">
                    <img style="width:500px" src="files/css/image/Here2 GPS receiver.jpg"/>
                </figure>
            </ul>
        </td>
    </tr>
    <tr>
        <td>
            <ul>
                <li><b>Jetson Nano</b></li>
                <li>The Jetson Nano, serving as a Companion Computer, handles complex tasks like AI processing, enhancing the Pixhawk's basic control functions.</li>
                <li>It's a powerful, small-sized computer designed for AI and robotics, supporting advanced neural networks and deep learning.</li>
                <li>With its high computational power and low power consumption, the Jetson Nano is ideal for intensive tasks in autonomous systems.</li>
                <figure align="center">
                    <img style="width:500px" src="files/css/image//nvidia-jetson-nano-development-kit.jpg"/>
                </figure>
            </ul>
        </td>
    </tr>
    <tr>
        <td>
            <ul>
                <li><b>Slamtec RPLiDAR A3</b></li>
                <li>LiDAR sensors, like the Slamtec RPLiDAR A3, are key in robotics for accurate environment mapping and navigation.</li>
                <li>They excel in obstacle detection and safety, with the RPLiDAR A3 offering a balance of range, accuracy, and affordability.</li>
                <li>Its 360-degree field of view and rapid scanning make it ideal for autonomous systems, particularly in cost-sensitive applications.</li>
                <figure align="center">
                    <img style="width:500px" src="files/css/image/Slamtec RPLiDAR A3.jpg"/>
                </figure>
            </ul>
        </td>
    </tr>
    <tr>
        <td>
            <ul>
                <li><b>Intel RealSense D455 Depth Camera</b></li>
                <li>The camera in our system is integral for perception, providing essential visual data for the rover's decision-making processes.</li>
                <li>Utilizing different types like RGB, tracking, and depth cameras, it captures detailed environmental information for various robotic tasks.</li>
                <li>The Intel RealSense D435 camera combines depth sensing and high-resolution imagery, aiding in precise navigation and object detection.</li>
                <figure align="center">
                    <img style="width:500px" src="files/css/image/Intel RealSense D455 Depth Camera.jpg"/>
                </figure>
            </ul>
        </td>
    </tr>
</table>

	<br>
        <center><h1>Software</h1></center>
<table align="center" width="850px">
    <tr>
        <td>
            <ul>
                <li><b>Mission Planner</b></li>
                <li>Mission Planner is a ground control software for ArduPilot vehicles, offering tools for configuration, planning, and monitoring.</li>
                <li>It provides an intuitive interface for setting up missions, calibrating sensors, and updating firmware within a Windows environment.</li>
                <li>The software supports real-time telemetry, waypoint creation, and automated mission execution for effective vehicle management.</li>
                <figure align="center">
                    <img style="width:500px" src="files/css/image/Mission Planner.jpg"/>
                </figure>
            </ul>
        </td>
    </tr>
    <tr>
        <td>
            <ul>
                <li><b>OpenRouteService API</b></li>
                <li>OpenRouteService API, based on OpenStreetMap, provides diverse routing solutions for different transportation modes including driving and cycling.</li>
                <li>Chosen for its accuracy, free usage, and unlimited API calls, it effectively generates waypoints for navigation in varied environments.</li>
                <li>It offers features like geocoding, reverse geocoding, and folium maps for comprehensive route planning and spatial analysis.</li>
                <figure align="center">
                    <img style="width:500px" src="files/css/image/Folium map of openrouteservice.jpg"/>
                </figure>
            </ul>
        </td>
    </tr>
    <tr>
        <td>
            <ul>
                <li><b>Semantic Segmentation</b></li>
                <li>Semantic segmentation models analyze camera feeds to classify objects and surfaces, enhancing the rover's environmental understanding.</li>
                <li>The camera's depth data is crucial for curb detection and, combined with LiDAR, helps construct a comprehensive 3D map for navigation.</li>
                <li>These models aid in real-time path planning by differentiating between traversable and non-traversable areas, ensuring safe navigation.</li>
                <figure align="center">
                    <img style="width:500px" src="files/css/image/Sematic segmentation 1.jpg"/>
                </figure>
                <figure align="center">
                    <img style="width:500px" src="files/css/image/sematic segmentation 2.jpg"/>
                </figure>
            </ul>
        </td>
    </tr>
</table>

	
	<hr>
	<table align=center width=850px>
		<center><h1>Results</h1></center>
		<tr>
			<td>
				<ul>
					<li>Our advanced rover system boasts an impressive capability to accurately detect objects positioned ahead of it, facilitating intelligent decision-making for obstacle avoidance.</li> 
					<li>Leveraging this capability, it can dynamically maneuver towards the nearest path, be it on the left or right, ensuring seamless navigation in complex environments.</li>
					<li>Moreover, the rover diligently adheres to a predetermined list of waypoints, meticulously following its navigation path with precision and reliability.</li> 
					<li>By seamlessly integrating object detection, adaptive path selection, and waypoint-guided navigation, our rover consistently reaches its desired end goal, making it a highly professional and efficient solution for a wide range of applications.</li>
					<li>The rover "Agni" employs semantic segmentation as a key navigational tool to ensure adherence to the road while avoiding off-road areas like grass.</li>
					<li>This advanced technique involves labeling various segments of the road environment, where each label is assigned a unique numerical identifier.</li>
					<li>These identifiers are crucial for the rover's understanding and interpretation of its surroundings.</li> 
					<li>Consequently, Agni is able to autonomously navigate by processing these labels, distinguishing between different areas such as the road and grass, thereby facilitating effective and safe autonomous navigation.</li>
					<br>
					<figure style="text-align: center;">
                                             <img class="round" style="width:350px; display: block; margin: auto;" src="files/css/image/Screenshot_20230525_181311_Gallery (1).jpg"/>
                                             <figcaption><b>AGNI completing the navigation for its path trace and arriving to its end point</b></figcaption>

                                                 <br><br>
    
                                            <img class="round" style="width:350px; display: block; margin: auto;" src="files/css/image/Screenshot_20230525_181628_Gallery (1).jpg"/>
                                        </figure>
					<figcaption align="center"><b>AGNI driving autonomously at the UCI campus</b></figcaption>

					          <br><br>
					    <img class="round" style="width:350px; display: block; margin: auto;" src="files/css/image/Screenshot (42).png"/>
                                        </figure>
					<figcaption align="center"><b>AGNI tracing its path on the road and starying away from the grass</b></figcaption>

					           <br><br>
					
<div class="resource-list">
    <h2>Youtube Videos</h2>
    <ul>
        <li><a href="https://www.youtube.com/watch?v=jTv6dk9j2JI" target="_blank">20 waypoints with object avoidance</a></li>
	<li><a href="https://www.youtube.com/watch?v=JtF-74tIR2g" target="_blank"> Semantic Segmentation</a></li>    
        <li><a href="https://www.youtube.com/watch?v=-MjjNYNSlf0" target="_blank">Wave point attempt</a></li>
        <li><a href="https://www.youtube.com/shorts/ye08_Nmhqas" target="_blank">Object Avoidance initial</a></li>
        <li><a href="https://www.youtube.com/watch?v=VVnbwAQF_hk&t=1s"_blank">Object Avoidance</a></li>
        <li><a href="https://www.youtube.com/shorts/3u-YGskfcaE" target="_blank">Object detection attempt</a></li>
        <li><a href="https://www.youtube.com/shorts/kCvzaAcpXC0" target="_blank">Object detection</a></li>
        <li><a href="https://www.youtube.com/shorts/CeqQycyqet4" target="_blank">Rover Movement</a></li>
	    <li><a href="https://www.youtube.com/watch?v=iRgux_9Lnx4" target="_blank"> Fail Attempt</a></li>
	    <li><a href="https://www.youtube.com/shorts/OGabWa8hgZo" target="_blank"> Failed Attempt</a></li>
	    <li><a href="https://www.youtube.com/shorts/aZOSRmImusE" target="_blank"> Dancing Rover</a></li>
        <li><a href="https://www.youtube.com/shorts/4FKzNsXotM0" target="_blank"> Random Movements</a></li>
    </ul>
</div>
				</ul>
			
			</td>
		</tr>
	</table>

	<br>


	<!-- <center> <h3> Distance between Adjacent Buildings </h3>
	<img class="round" style="width:1000px" src="assets/images/DistanceModule.png"/>
	<p>This module provide us the distance between two adjacent buildings. We sampled the images from the videos captured by UAV and perform panoptic segmentation using state-of-art deep learning model, eliminating vegetation (like trees) from the images. The masked images are then fed to a state-of-the art image-based 3D reconstruction library which outputs a dense 3D point cloud. We then apply RANSAC for fitting planes between the segmented structural point cloud. Further, the points are sampled on these planes to calculate the distance between the adjacent buildings at different locations.</p>
	</center>
	<br> -->


	<!-- <center> <h3> Results: Distance between Adjacent Buildings </h3>
	<img class="round" style="width:800px" src="assets/images/Results-DistanceModule-1.png"/>
	<p> Sub-figures (a), (b) and (c) and (d), (e) and (f) represent the implementation of plane fitting using piecewise-RANSAC in different views for two subject buildings.</p>
	
	</center>
	<br> -->




	<!-- <center> <h3> Plan Shape and Roof Area Estimation </h3>
	<img class="round" style="width:1000px" src="assets/images/PlanShape&RoofareaEstimation.png"/>
	<p>This module provides information regarding the shape and roof area of the building. We segment the roof using a state-of-the-art semantic segmentation deep learning model. We also subjected the input images to a pre-processing module that removes distortions from the wide-angle images. Data augmentation was used to increase the robustness and performance. Roof Area was calculated using the focal length of the camera, the height of the drone from the roof and the segmented mask area in pixels.
</p>
	</center>
	<br>



	<center> <h3> Results: Plan Shape and Roof Area Estimation </h3>
	<img class="round" style="width:800px" src="assets/images/Results-PlanShape&RoofareaEstimation-1.png"/>
	<p>This figure represents the roof segmentation results for 4 subject buildings.</p>
	
	</center>
	<br>


	<center> <h3> Roof Layout Estimation </h3>
	<img class="round" style="width:1000px" src="assets/images/RoofLaoutEstimation.png"/>
	<p>This module provides information about the roof layout. Since it is not possible to capture the whole roof in a single frame specially in the case of large sized buildings, we perform large scale image stitching of partially visible roofs followed by NSE detection and roof segmentation.

</p>
	</center>
	<br> -->



	<!-- <center> <h3> Results: Roof Layout Estimation </h3>
	<img class="round" style="width:800px" src="assets/images/imagestitchingoutput.jpeg"/>
	<p>Stitched Image</p>
	<img class="round" style="width:800px" src="assets/images/roofmask.png"/>
	<p>Roof Mask</p>
	<img class="round" style="width:800px" src="assets/images/objectmask.png"/>
	<p>Object Mask</p>
	
	</center>
	<br> -->




<!--

	<center> <h1> Resources </h1>
	<table width="100%" style="margin: 20pt auto; text-align: center;">
	      <tr>
				<td width="33%" valign="middle">
				    <center>
					<a href="https://github.com/atmacvit/meronymnet" target="_blank"
					   class="imageLink"><img src="./resources/Octocat.png" ,=, width="75%" /></a><br /><br />
					<a href="https://github.com/atmacvit/meronymnet" target="_blank">Code</a>
				    </center>
				</td>
				<td width="33%" valign="middle">
				    <center>
					<a href="https://drive.google.com/file/d/1NnY4tcV1wnlSWMzT_Ae6hH6v5l8GCIrX/view?usp=sharing" target="_blank"
					   class="imageLink"><img src="./resources/Paper_crop.png" ,=, width="75%" /></a><br /><br />
					<a href="https://drive.google.com/file/d/1NnY4tcV1wnlSWMzT_Ae6hH6v5l8GCIrX/view?usp=sharing" target="_blank">Paper</a>
				    </center>

				</td>


			       <td width="33%" valign="middle">
				    <center>
					<a href="" target="_blank"
					   class="imageLink"><img src="./resources/Supp_crop.png" ,=, width="75%" /></a><br /><br />
					<a href="" target="_blank">Supplementary</a>
				    </center>
				</td>
	      </tr>
    	</table>
	</center>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br> -->
<hr>
<center> <h1> Contact </h1>
	<p>If you have any questions, please reach out to any of the above mentioned authors.</p>
	</center>

</body>
</html>
