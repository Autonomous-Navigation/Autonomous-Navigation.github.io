<!DOCTYPE html>
<html>
<head>
	<title>AGNI</title>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="YOUR_GTAG_JS_SOURCE"></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>
<body>
	<br>
	<center>
		<span style="font-size:36px">AGNI : Autonomous Ground Navigation Initiative</span>
		<table align="center" width="1000px">
			<tr>
				<td>
					<table align="center" width="1000px" cellspacing="5px">
						<tr>
							<td align="center" width="150px">
								<span style="font-size:20px"><a href="https://www.linkedin.com/in/arjun-sivakumar-2459701a0/" target="_blank">Arjun Sivakumar<sup>1</sup>*</a></span>
							</td>
							<td align="center" width="150px">
								<span style="font-size:20px"><a href="https://www.linkedin.com/in/bhumil-depani-779a32147/" target="_blank">Bhumil Depani<sup>1</sup>*</a></span>
							</td>
							<td align="center" width="150px">
								<span style="font-size:20px"><a href="https://www.linkedin.com/in/ricky-bevan-babu-290ba9190/" target="_blank">Ricky Bevan Babu<sup>1</sup>*</a></span>
							</td>
						</tr>
					</table>
				</td>
			</tr>
		</table>
		<table align="center" width="800px">
                <tr>
                    <td>
                        <center>
                            <img class="round" style="width:350px" src="files/css/image/Screenshot_20230524_115532_Gallery-removebg-preview.png"/>
                        </center>
                    </td>
                </tr>
            </table>

		<br>

		<table align="center" width="850px">
		<h1 style="text-align:center">Motivation</h1>
		<tr>
			<td>
				<ul>
					<li>Traversing rough terrain: Rovers are often deployed in environments with rugged landscapes such as rocky terrains, steep slopes, or even extraterrestrial surfaces. The inability to navigate such rough terrains restricts their access to important locations, limiting their ability to explore and gather valuable data. Overcoming this limitation would enable rovers to venture into previously inaccessible areas, unlocking new scientific discoveries and expanding our understanding of the natural and extraterrestrial worlds.</li>
					<li>Inability to climb stairs: Urban and indoor environments often feature staircases or other vertical obstacles that pose significant challenges for rovers. This limitation hinders their ability to perform tasks in buildings, disaster-stricken areas, or even search and rescue operations. Enabling rovers to overcome this obstacle and navigate staircases would greatly expand their potential applications, allowing them to access areas that are currently inaccessible to conventional wheeled rovers.</li>
					<li>Lack of overall visibility: Rovers typically rely on cameras and sensors to perceive their surroundings. However, these systems have limitations in terms of range, resolution, and field of view. The restricted visibility prevents rovers from comprehensively assessing their environment, which can lead to missed opportunities or potential hazards. Developing technologies that provide rovers with a broader and more detailed understanding of their surroundings would greatly enhance their situational awareness and decision-making capabilities.</li>
					
					
				</ul>
			</td>
		</tr>
	</table>

	<hr>
	<table align="center" width="850px">
		<h1 style="text-align:center">Overview</h1>
		<tr>
			<td>
				<ul>
					<li>Our project focuses on developing an autonomous ground navigation system for a robot that can serve as an advanced delivery bot capable of delivering packages to various locations. With a range of innovative features, our solution aims to revolutionize the delivery industry by enabling efficient and intelligent autonomous delivery operations.</li>
					<li><b>Autonomous Navigation</b>: The robot is equipped with state-of-the-art navigation capabilities, allowing it to autonomously navigate to designated destinations. It utilizes advanced algorithms and sensor data to make intelligent decisions and safely navigate through various terrains, obstacles, and road conditions.</li>
					<li><b>Object Detection</b>: Our system incorporates sophisticated object detection techniques that enable the robot to detect and recognize objects in its surroundings. This capability ensures safe and reliable navigation by identifying potential obstacles or hazards along its path.</li>
					<li><b>Sign Detection and Understanding</b>: Our system incorporates advanced computer vision algorithms to detect and interpret road signs and other relevant signage. This feature enables the robot to comprehend traffic regulations, navigate through intersections, and follow specific instructions, ensuring compliance with traffic rules and enhancing overall safety.</li>
					<li><b>End-to-End Delivery</b>: Our solution provides end-to-end delivery capabilities, from pickup to final destination. The robot can autonomously handle package collection, secure storage, route planning, and efficient delivery, eliminating the need for human intervention throughout the process.</li>
				        <li>By combining cutting-edge technologies, including advanced navigation algorithms, computer vision, and intelligent decision-making, our autonomous ground navigation system offers a comprehensive and reliable solution for autonomous delivery operations. With its ability to detect objects, understand road conditions, and navigate autonomously, our robot has the potential to revolutionize the delivery industry, providing efficient and safe deliveries to diverse locations.</li>
				</ul>
			</td>
		</tr>
	</table>
	<br>
	<hr>
	<table align=center width=850px>
		<center><h1>Modules</h1></center>
		<tr>
			<td> 
				<ul>
					<li><b>Rover</b>:  The rover is the physical vehicle or robot that is designed for autonomous ground navigation. It serves as the main component of the rover ecosystem and is responsible for carrying out various tasks and missions. The rover consists of several subsystems and components that work together to enable its operation and functionality.</li>
					<li><b>Subsystems of Rover</b>:</li>
                                        <li><b>Motor</b>: Motors provide the necessary propulsion for the rover to move. They generate rotational motion that drives the wheels or tracks of the rover.</li.
					<li><b>ESC (Electronic Speed Controller)</b>: ESCs control the speed and direction of the motors by regulating the power supply to them.</li>
					<li><b>Flight Controller</b>: The flight controller acts as the brain of the rover, responsible for processing sensor data, executing control algorithms, and generating commands to control the rover's movements. In the context of rovers, the flight controller, such as Pixhawk, is used as a central processing unit.</li>
					<li><b>Frame</b>: The frame refers to the physical structure or chassis of the rover that holds all the components together. It provides stability and protection to the internal components.</li>
					<li><b>Sensors</b>: Sensors play a crucial role in the rover's perception and navigation capabilities. They can include cameras, LiDAR, GPS, inertial measurement units (IMUs), and other environmental sensors. These sensors gather data about the rover's surroundings and provide valuable information for decision-making and navigation.</li>
						<figure align=center>
							<img style="width:500px" src="files/css/image/rplidar-a2-pixhawk.jpg"/>
							<figcaption align="center">Lidar and Pixhawk
							</figcaption>
						</figure>
					<br>
				<li> <b>Ground Control System</b>:</li>
                                        <li>The ground control system (GCS) is a vital part of the rover ecosystem, enabling remote control, monitoring, and communication with the rover. It consists of hardware and software components that allow operators or users to interact with the rover and oversee its operation.</li>
				        <li><b>Ground Control System Components</b>:</li>
                                        <li><b>Hardware</b>: The hardware components of the ground control system include devices such as laptops, tablets, smartphones, or dedicated ground control stations. These devices provide a user interface for operators to interact with the rover and monitor its status.</li>
					<li><b>Software</b>: The software component of the ground control system includes applications or programs that facilitate communication, mission planning, and monitoring of the rover. Examples of popular ground control software are Mission Planner (Windows) and QGroundControl (available for IOS, Linux, and Windows).</li>
						<figure align=center>
							<img style="width:500px" src="files/css/image//nvidia-jetson-nano-development-kit.jpg"/>
							<figcaption align="center">Jetson Nano
							</figcaption>
						</figure>
					<br>

				        <li><b>Communication Systems</b>:</li>
                                        <li>Communication systems are responsible for establishing a reliable connection between the rover and the ground control system. They enable real-time data transmission, command execution, and telemetry monitoring.</li>
				        <li><b>Communication System Components</b>:</li>
                                        <li><b>Hardware</b>: The hardware components of the communication system include telemetry systems and antennas. Telemetry systems establish a wireless link between the rover and the ground control system, allowing bidirectional data transmission. Antennas help in transmitting and receiving signals over the communication link.</li>
				        <li><b>Software</b>: The software component of the communication system involves protocols and communication frameworks that facilitate data exchange between the rover and the ground control system. MAVLink is a popular communication protocol used in the drone and rover ecosystem. It provides a lightweight and efficient means of communication between different components of the system.</li>
						<br><br>
							<figure align=center>
								<img align="center" style="width:400px" src="files/css/image/rover.png"/>
								<figcaption align="center">Telemetry System</figcaption>
							</figure>
					</li>
					<br><br>

				</ul>
			
			</td>
		</tr>
	</table>

	<br>

	
	<hr>
	<table align=center width=850px>
		<center><h1>Results</h1></center>
		<tr>
			<td>
				<ul>
					<p>Our advanced rover system boasts an impressive capability to accurately detect objects positioned ahead of it, facilitating intelligent decision-making for obstacle avoidance. Leveraging this capability, it can dynamically maneuver towards the nearest path, be it on the left or right, ensuring seamless navigation in complex environments. Moreover, the rover diligently adheres to a predetermined list of waypoints, meticulously following its navigation path with precision and reliability. By seamlessly integrating object detection, adaptive path selection, and waypoint-guided navigation, our rover consistently reaches its desired end goal, making it a highly professional and efficient solution for a wide range of applications.
					</p>
					<br>
					<figure style="text-align: center;">
                                             <img class="round" style="width:350px; display: block; margin: auto;" src="files/css/image/Screenshot_20230525_181311_Gallery.jpg"/>
                                             <figcaption><b>Rover completing the navigation for its path trace and arriving to its end point</b></figcaption>

                                                 <br><br>
    
                                            <img class="round" style="width:350px; display: block; margin: auto;" src="files/css/image/Screenshot_20230525_181628_Gallery.jpg"/>
                                        </figure>
					<figcaption align="center">Drive links to our Rover videos</figcaption>
<div class="resource-list">
    <h2>Drive Files</h2>
    <ul>
        <li><a href="https://drive.google.com/file/d/1-7R8MiwkqgUmnGQT9GzbLzs_xwGtbviH/view?usp=share_link" target="_blank">20 waypoints with object avoidance</a></li>
        <li><a href="https://drive.google.com/file/d/1QeCMjja01w7XSonY5bBtMyiWnGzmgiOU/view?usp=share_link" target="_blank">Wav point attempt</a></li>
        <li><a href="https://drive.google.com/file/d/1BlXFK72tt306m48O6NWDCWm0XuJWaoxm/view?usp=share_link" target="_blank">Object Avoidance initial</a></li>
        <li><a href="https://drive.google.com/file/d/17rmmRaS4YW2JoCl3bAPSL5nZFMvTsL9X/view?usp=share_link" target="_blank">Object Avoidance</a></li>
        <li><a href="https://drive.google.com/file/d/12w3uVLgukv0i6RvJb3jTcfNHxMVF64Wk/view?usp=share_link" target="_blank">Object detection attempt</a></li>
        <li><a href="https://drive.google.com/file/d/10_ZGdcZm9XN_RPEm-HgaQVjh_6CSFJFl/view?usp=share_link" target="_blank">Object detection</a></li>
        <li><a href="https://drive.google.com/file/d/10JHYVdr5Zp4dh2Pzi3IeoohXCnBTD0B9/view?usp=share_link" target="_blank">Rover Movement</a></li>
	    <li><a href="https://drive.google.com/file/d/10M9ResRwMfbvym7O6DodsoybvWt0YOCV/view?usp=share_link" target="_blank"> Fail Attempt</a></li>
	    <li><a href="https://drive.google.com/file/d/1NaVFk99b0gmGeQvN-JlehZR2JNdQwNEm/view?usp=share_link" target="_blank"> Failed Attempt</a></li>
	    <li><a href="https://drive.google.com/file/d/10_ODTA-uM8xP6mVVkPuTNFtrrh8JrGBz/view?usp=share_link" target="_blank"> Dancing Rover</a></li>
        <li><a href="https://drive.google.com/file/d/10VbuO5ytRB9siMFNQxQKMc0PFY9QrxPH/view?usp=share_link" target="_blank"> Random Movements</a></li>
    </ul>
</div>
				</ul>
			
			</td>
		</tr>
	</table>

	<br>


	<!-- <center> <h3> Distance between Adjacent Buildings </h3>
	<img class="round" style="width:1000px" src="assets/images/DistanceModule.png"/>
	<p>This module provide us the distance between two adjacent buildings. We sampled the images from the videos captured by UAV and perform panoptic segmentation using state-of-art deep learning model, eliminating vegetation (like trees) from the images. The masked images are then fed to a state-of-the art image-based 3D reconstruction library which outputs a dense 3D point cloud. We then apply RANSAC for fitting planes between the segmented structural point cloud. Further, the points are sampled on these planes to calculate the distance between the adjacent buildings at different locations.</p>
	</center>
	<br> -->


	<!-- <center> <h3> Results: Distance between Adjacent Buildings </h3>
	<img class="round" style="width:800px" src="assets/images/Results-DistanceModule-1.png"/>
	<p> Sub-figures (a), (b) and (c) and (d), (e) and (f) represent the implementation of plane fitting using piecewise-RANSAC in different views for two subject buildings.</p>
	
	</center>
	<br> -->




	<!-- <center> <h3> Plan Shape and Roof Area Estimation </h3>
	<img class="round" style="width:1000px" src="assets/images/PlanShape&RoofareaEstimation.png"/>
	<p>This module provides information regarding the shape and roof area of the building. We segment the roof using a state-of-the-art semantic segmentation deep learning model. We also subjected the input images to a pre-processing module that removes distortions from the wide-angle images. Data augmentation was used to increase the robustness and performance. Roof Area was calculated using the focal length of the camera, the height of the drone from the roof and the segmented mask area in pixels.
</p>
	</center>
	<br>



	<center> <h3> Results: Plan Shape and Roof Area Estimation </h3>
	<img class="round" style="width:800px" src="assets/images/Results-PlanShape&RoofareaEstimation-1.png"/>
	<p>This figure represents the roof segmentation results for 4 subject buildings.</p>
	
	</center>
	<br>


	<center> <h3> Roof Layout Estimation </h3>
	<img class="round" style="width:1000px" src="assets/images/RoofLaoutEstimation.png"/>
	<p>This module provides information about the roof layout. Since it is not possible to capture the whole roof in a single frame specially in the case of large sized buildings, we perform large scale image stitching of partially visible roofs followed by NSE detection and roof segmentation.

</p>
	</center>
	<br> -->



	<!-- <center> <h3> Results: Roof Layout Estimation </h3>
	<img class="round" style="width:800px" src="assets/images/imagestitchingoutput.jpeg"/>
	<p>Stitched Image</p>
	<img class="round" style="width:800px" src="assets/images/roofmask.png"/>
	<p>Roof Mask</p>
	<img class="round" style="width:800px" src="assets/images/objectmask.png"/>
	<p>Object Mask</p>
	
	</center>
	<br> -->




<!--

	<center> <h1> Resources </h1>
	<table width="100%" style="margin: 20pt auto; text-align: center;">
	      <tr>
				<td width="33%" valign="middle">
				    <center>
					<a href="https://github.com/atmacvit/meronymnet" target="_blank"
					   class="imageLink"><img src="./resources/Octocat.png" ,=, width="75%" /></a><br /><br />
					<a href="https://github.com/atmacvit/meronymnet" target="_blank">Code</a>
				    </center>
				</td>
				<td width="33%" valign="middle">
				    <center>
					<a href="https://drive.google.com/file/d/1NnY4tcV1wnlSWMzT_Ae6hH6v5l8GCIrX/view?usp=sharing" target="_blank"
					   class="imageLink"><img src="./resources/Paper_crop.png" ,=, width="75%" /></a><br /><br />
					<a href="https://drive.google.com/file/d/1NnY4tcV1wnlSWMzT_Ae6hH6v5l8GCIrX/view?usp=sharing" target="_blank">Paper</a>
				    </center>

				</td>


			       <td width="33%" valign="middle">
				    <center>
					<a href="" target="_blank"
					   class="imageLink"><img src="./resources/Supp_crop.png" ,=, width="75%" /></a><br /><br />
					<a href="" target="_blank">Supplementary</a>
				    </center>
				</td>
	      </tr>
    	</table>
	</center>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br> -->
<hr>
<center> <h1> Contact </h1>
	<p>If you have any questions, please reach out to any of the above mentioned authors.</p>
	</center>

</body>
</html>
